import streamlit as st
import time
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForQuestionAnswering

# --- 1. CONFIGURATION & CSS (DESIGN GEMINI) ---
st.set_page_config(page_title="Projet SQuAD ", page_icon="üß†", layout="wide")

st.markdown("""
<style>
    /* IMPORT FONTS */
    @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap');
    .stApp { background-color: #F8FAFC; font-family: 'Inter', sans-serif; }

    /* SIDEBAR BLEUE */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #0F172A 0%, #1E40AF 100%);
        color: white;
    }
    [data-testid="stSidebar"] h1, [data-testid="stSidebar"] h2, [data-testid="stSidebar"] label, [data-testid="stSidebar"] p, [data-testid="stSidebar"] span {
        color: #E2E8F0 !important;
    }
    .stTextArea textarea {
        border-radius: 10px;
        background-color: rgba(255,255,255,0.95);
        color: #1e293b;
        font-size: 0.9rem;
    }

    /* CHAT BUBBLES */
    .user-bubble {
        background-color: #EFF6FF; color: #1E293B; padding: 15px 20px;
        border-radius: 20px 20px 5px 20px; margin-bottom: 15px; text-align: right;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05); max-width: 80%; margin-left: auto;
    }
    
    /* INPUT & BOUTON ROND */
    .stTextInput input {
        border-radius: 30px !important; padding: 15px 25px !important;
        border: 1px solid #4F46E5 !important;
    }
    div.stButton > button {
        border-radius: 50% !important; width: 55px; height: 55px;
        background: linear-gradient(135deg, #2563EB 0%, #4F46E5 100%);
        color: white; border: none; display: flex; align-items: center; justify-content: center;
        font-size: 24px; padding: 0 !important;
    }

    div.stButton > button:hover { 
        transform: scale(1.1); 
        box-shadow: 0 6px 8px rgba(0,0,0,0.3);
            
    }
</style>
""", unsafe_allow_html=True)

# --- 2. BASE DE CONNAISSANCE ---
knowledge_base = {
    "‚úçÔ∏è Personnalis√©": "",
    "üéì Master DATASCALE": """Le M2 DataScale forme des experts dot√©s d‚Äôune double comp√©tence en ing√©nierie et analyse des donn√©es. Il couvre l‚Äôadministration de grands volumes de donn√©es, l‚Äôanalyse de donn√©es de capteurs, la protection de la vie priv√©e et la pr√©diction de ph√©nom√®nes complexes, en s‚Äôappuyant sur la fouille de donn√©es, le machine learning et l‚ÄôIA.
La formation pr√©pare √† des m√©tiers vari√©s : Data Engineer, Data Scientist, IA Analyst, CDO, DBA ou Urbaniste SI.
Le programme combine un tronc commun, des options sp√©cialis√©es et des modules de professionnalisation (projets et s√©minaires).
Responsables : Mustapha Lebbah et Zoubida Kedad-Cointot.""",
    "üêç Langage Python": """Python est un langage de programmation interpr√©t√©, multiparadigme et multiplateformes. Il favorise la programmation imp√©rative structur√©e, fonctionnelle et orient√©e objet. Il a √©t√© cr√©√© par Guido van Rossum et publi√© pour la premi√®re fois en 1991.""",
    "üóº Tour Eiffel": """La tour Eiffel est une tour de fer puddl√© de 330 m de hauteur situ√©e √† Paris. Construite par Gustave Eiffel et ses collaborateurs pour l'Exposition universelle de Paris de 1889, elle est devenue le symbole de la capitale fran√ßaise."""
}

# --- 3. CHARGEMENT HYBRIDE (OPTIMIS√â MAC M1/M2/M3) ---
@st.cache_resource
def load_model(model_info):
    path = model_info["path"]
    model_type = model_info["type"]
    
    # 1. D√âTECTION INTELLIGENTE DU MAT√âRIEL
    if torch.cuda.is_available():
        device = "cuda" # Pour PC avec NVIDIA
    elif torch.backends.mps.is_available():
        device = "mps"  # <--- C'EST ICI POUR VOTRE MAC ! üçé
    else:
        device = "cpu"  # Sinon, on utilise le processeur classique
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(path)
        
        # S√âLECTION DU BON ARCHITECTURE
        if model_type == "seq2seq":
            # Pour T5
            model = AutoModelForSeq2SeqLM.from_pretrained(path).to(device)
        else:
            # Pour BERT et ALBERT (Question Answering)
            model = AutoModelForQuestionAnswering.from_pretrained(path).to(device)
            
        return tokenizer, model, device, None
    except Exception as e:
        return None, None, "cpu", str(e)

# --- 4. SIDEBAR : CONFIGURATION ---
with st.sidebar:
    st.title("‚öôÔ∏è Configuration")  # <--- TITRE RAJOUT√â ICI
    
    model_options = {
        "üèÜ Model T5 ": {
            "path": "ciscom224/fine-tuning-t5-small-model-for-squad", 
            "type": "seq2seq"
        },
        "ü¶Å Model BERT": {
            "path": "bert-large-uncased-whole-word-masking-finetuned-squad",
            "type": "extractive"
        },
        "‚ö° Model ALBERT ": {
            "path": "models/albert/checkpoint-8000", 
            "type": "extractive"
        }
    }
    
    selected_name = st.selectbox("Choisir le mod√®le", list(model_options.keys()))
    current_info = model_options[selected_name]

    with st.spinner(f"Chargement de {selected_name}..."):
        tokenizer, model, device, err = load_model(current_info)
    
    if err:
        st.error(f"Erreur de chargement : {err}")
    else:
        st.success(f"Pr√™t ({device.upper()})")
        if current_info["type"] == "seq2seq":
            st.caption("üìù Mode : G√©n√©ration (Reformulation)")
        else:
            st.caption("üîç Mode : Extraction (Surlignage)")

    st.markdown("---")

    # GESTION DU CONTEXTE
    st.markdown("### Document Source")
    selected_topic = st.selectbox("Sujets Pr√©d√©finis :", list(knowledge_base.keys()))
    
    if "context" not in st.session_state:
        st.session_state.context = knowledge_base["üéì Master DATASCALE"]
        
    # Mise √† jour si changement de s√©lection
    if selected_topic != "‚úçÔ∏è Personnalis√© (Vide)" and knowledge_base[selected_topic] != st.session_state.context:
        st.session_state.context = knowledge_base[selected_topic]
        
    context_text = st.text_area("Contenu du document", value=st.session_state.context, height=300)
    st.session_state.context = context_text

# --- 5. TITRE PRINCIPAL & CHAT ---
st.title("üß† Projet SQuAD") # <--- TITRE RAJOUT√â ICI

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Bonjour ! Je suis pr√™t √† analyser votre texte. Posez une question."}]

st.markdown("<div class='chat-container'>", unsafe_allow_html=True)
for msg in st.session_state.messages:
    if msg["role"] == "user":
        st.markdown(f"""<div class="user-bubble"><strong>Vous</strong><br>{msg["content"]}</div>""", unsafe_allow_html=True)
    else:
        with st.chat_message("assistant", avatar="‚ú®"):
            st.markdown(msg["content"])
st.markdown("</div>", unsafe_allow_html=True)

# --- 6. ZONE DE SAISIE ---
st.markdown("---")
with st.form(key="chat_input_form", clear_on_submit=True):
    col_input, col_btn = st.columns([8, 1])
    with col_input:
        user_input = st.text_input("Votre question...", placeholder="Posez votre question...", label_visibility="collapsed")
    with col_btn:
        submit_btn = st.form_submit_button("‚û§")

# --- 7. LOGIQUE D'INF√âRENCE UNIFI√âE ---
if submit_btn and user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.chat_message("assistant", avatar="‚ú®"):
        message_placeholder = st.empty()
        full_response = ""

        if not context_text or len(context_text) < 5:
            final_answer = "‚ö†Ô∏è Veuillez fournir un contexte plus long."
        else:
            try:
                # --- CAS A : T5 (G√©n√©ratif) ---
                if current_info["type"] == "seq2seq":

                    prompt = f"question: {user_input} context: {context_text}"
                    try:
                        # Tokenization sur le bon device (CPU ou GPU)
                        inputs = tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True).to(device)
                        
                        # G√©n√©ration
                        outputs = model.generate(
                            inputs.input_ids,
                            max_length=128,   # Assurez-vous que c'est assez grand
                            num_beams=4,      # Augmentez un peu (4 -> 5) pour qu'il explore plus
                            length_penalty=2.5, 
                            early_stopping=True,
                            no_repeat_ngram_size=2
                        )
                        raw_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
                        
                        # Gestion du "Unanswerable"
                        if "unanswerable" in raw_answer.lower():
                            final_answer = "‚ùå D√©esol√©!  Cette question est hors contexte."
                        else:
                            final_answer = raw_answer
                            
                    except Exception as e:
                        final_answer = f"Erreur : {e}"

                # --- CAS B : BERT / ALBERT (Extractif) ---
                else:
                    inputs = tokenizer(user_input, context_text, return_tensors="pt", max_length=512, truncation=True).to(device)
                    
                    with torch.no_grad():
                        outputs = model(**inputs)
                    
                    # Logique : On prend le meilleur d√©but et la meilleure fin
                    start_idx = torch.argmax(outputs.start_logits)
                    end_idx = torch.argmax(outputs.end_logits)
                    
                    # V√©rification SQuAD v2 (Si fin < d√©but, c'est impossible -> pas de r√©ponse)
                    if end_idx < start_idx:
                        final_answer = "üö´ D√©sol√©!!! Pas de r√©ponse trouv√©e dans ce contexte."
                    else:
                        tokens = inputs.input_ids[0][start_idx : end_idx + 1]
                        final_answer = tokenizer.decode(tokens, skip_special_tokens=True)
                        
                        # Nettoyage
                        final_answer = final_answer.replace("[CLS]", "").replace("[SEP]", "").strip()
                        if not final_answer: 
                             final_answer = "üö´ Pas de r√©ponse trouv√©e."

            except Exception as e:
                final_answer = f"Erreur technique : {e}"

        # Animation d'√©criture
        for chunk in final_answer.split(" "):
            full_response += chunk + " "
            time.sleep(0.05)
            message_placeholder.markdown(full_response + "‚ñå")
        message_placeholder.markdown(full_response)

    st.session_state.messages.append({"role": "assistant", "content": full_response})
    st.rerun()