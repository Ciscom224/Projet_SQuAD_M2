# ğŸ§  Fine-tuning de ModÃ¨les de Question-Answering (SQuAD)

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![Hugging Face](https://img.shields.io/badge/Hugging%20Face-Transformers-orange)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-red)
![UVSQ](https://img.shields.io/badge/UVSQ-M2%20Datascale-green)

> **Projet Final - M2 Datascale - Fouille de DonnÃ©es** > UniversitÃ© de Versailles Saint-Quentin-en-Yvelines (UVSQ)

## ğŸ“– Description

Ce projet vise Ã  explorer, entraÃ®ner (fine-tuner) et dÃ©ployer des modÃ¨les de **Traitement Automatique du Langage Naturel (NLP)** capables de rÃ©pondre Ã  des questions basÃ©es sur un contexte donnÃ© (Question-Answering) .

Nous utilisons le jeu de donnÃ©es standard **SQuAD (Stanford Question Answering Dataset)** pour entraÃ®ner 3 architectures de modÃ¨les prÃ©-entraÃ®nÃ©s (Transformers). L'objectif est de comparer leurs performances et de fournir une interface utilisateur web interactive pour tester les modÃ¨les en temps rÃ©el.

## ğŸ¯ Objectifs

1.  **Les ModÃ¨les :** Comparaison de trois architectures distinctes : **T5** (GÃ©nÃ©ratif), **ALBERT** (OptimisÃ©) et **BERT** (Haute Performance) .
2.  **Analyse Comparative :** Ã‰valuer les modÃ¨les selon les mÃ©triques **F1-Score**, **Exact Match (EM)** et le **temps d'infÃ©rence** .
3.  **Interface Utilisateur :** DÃ©velopper une application Web (Streamlit) permettant aux utilisateurs de poser des questions sur leurs propres textes ou fichiers .
4.  **DÃ©ploiement :** Rendre l'application accessible via Hugging Face Spaces .

## ğŸ— Architecture du Projet

Le projet est structurÃ© pour sÃ©parer la phase de recherche (Notebooks) de la phase de production (App Web) :

```text
Projet_SQuAD/
â”‚
â”œâ”€â”€ ğŸ“‚ data/                  # DonnÃ©es brutes et prÃ©-traitÃ©es
â”œâ”€â”€ ğŸ“‚ notebooks/             # Contient le notebook principal d'analyse et d'entraÃ®nement
â”‚   â””â”€â”€ main.ipynb
â”‚
â”œâ”€â”€ ğŸ“‚ models/                # Sauvegarde des modÃ¨les fine-tunÃ©s (exclus du git via .gitignore)
â”œâ”€â”€ ğŸ“‚ app/                   # Code de l'application Web pour le dÃ©ploiement
â”‚   â”œâ”€â”€ app.py                # Point d'entrÃ©e Streamlit
â”‚   â””â”€â”€ utils.py              # Fonctions d'infÃ©rence
â”‚
â”œâ”€â”€ requirements.txt          # DÃ©pendances Python
â””â”€â”€ README.md                 # Documentation du projet
```
## ğŸ“¦ DÃ©pendances et RÃ´les Techniques

| BibliothÃ¨que | RÃ´le  |
| :--- | :--- |
| **`transformers`** (Hugging Face) |  Permet de charger les architectures (T5, DistilBERT, RoBERTa), les tokenizers et les pipelines de QA. |
| **`datasets`** | UtilisÃ© pour tÃ©lÃ©charger et gÃ©rer le dataset SQuAD de maniÃ¨re efficace et standardisÃ©e. |
| **`torch` (PyTorch)** | Framework de Deep Learning servant de backend pour les calculs tensoriels et l'optimisation. |
| **`streamlit`** | Framework permettant de crÃ©er l'interface utilisateur (Frontend) pour la dÃ©mo interactive. |
| **`evaluate` / `scikit-learn`** | Calcul des mÃ©triques de performance (Exact Match, F1-Score) pour valider les rÃ©sultats. |
| **`pandas` & `matplotlib`** | Manipulation des donnÃ©es et visualisation des rÃ©sultats comparatifs. |

## ğŸš€ Guide d'Installation et d'ExÃ©cution

Suivez ces Ã©tapes pour reproduire l'environnement de dÃ©veloppement et lancer l'application localement.

### 1. Cloner le dÃ©pÃ´t
```bash
git clone https://github.com/Ciscom224/Projet_SQuAD_M2.git

```
### 2. CrÃ©er un environnement virtuel
**Windows :**
```bash
python -m venv venv
.\venv\Scripts\activate
```
**Mac / Linux :**

```bash
python3 -m venv venv
source venv/bin/activate
```
## ğŸš€ Guide d'Installation et d'ExÃ©cution

* **Installation :**
    ```bash
    pip install -r requirements.txt
    ```

* **EntraÃ®nement :**
    ExÃ©cuter le notebook situÃ© dans `notebooks/` pour gÃ©nÃ©rer les modÃ¨les fine-tunÃ©s.

* **Lancement de l'App :**
    ```bash
    streamlit run app/app.py
    ```
## ğŸ“Š MÃ©triques d'Ã‰valuation

Pour Ã©valuer la performance de nos modÃ¨les sur le jeu de donnÃ©es SQuAD, nous utilisons les mÃ©triques standards suivantes :

* **Exact Match (EM) :** Mesure le pourcentage de prÃ©dictions qui correspondent **exactement** Ã  la rÃ©ponse attendue (mot pour mot). C'est une mÃ©trique trÃ¨s stricte (0 ou 1).
* **F1-Score :** Moyenne harmonique de la prÃ©cision et du rappel. Cette mÃ©trique est plus souple et Ã©value le chevauchement (overlap) des mots entre la rÃ©ponse prÃ©dite et la vÃ©ritÃ© terrain.
* **Temps d'infÃ©rence (Latence) :** Mesure du temps moyen nÃ©cessaire au modÃ¨le pour gÃ©nÃ©rer une rÃ©ponse. Cette mÃ©trique est cruciale pour Ã©valuer la viabilitÃ© du dÃ©ploiement en temps rÃ©el sur l'application Web.

## ğŸ‘¥ Auteurs
* **[PrÃ©nom Nom]** â€“ [GitHub](https://github.com/) | [LinkedIn](https://www.linkedin.com/)
* **[PrÃ©nom Nom]** â€“ [GitHub](https://github.com/) | [LinkedIn](https://www.linkedin.com/)
* **Mamadou CissÃ©**  â€“ [GitHub](https://github.com/Ciscom224) | [LinkedIn](https://www.linkedin.com/in/cissemamadou/)


**Encadrant :**
* [Nom du Professeur]